{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "35i33jL2c-U5"
   },
   "source": [
    "# Recommender Systems, NLP\n",
    "#### Author: Atanu Choudhury\n",
    "\n",
    "Environment: Python 3.6.5 and Jupyter Notebook\n",
    "\n",
    "Libraries used: \n",
    "* re (for regular expression) \n",
    "* json (to write json file)\n",
    "* re (for regular expression) \n",
    "* nltk 3.2.2 (Natural Language Toolkit)\n",
    "* nltk.tokenize (for tokenization)\n",
    "* nltk.collocations (for finding bigrams)\n",
    "* nltk.stem (for stemming)\n",
    "* collections (for defaultdict and Counter)\n",
    "* itertools (for chaining iterables)\n",
    "* TfidfVectorizer (to create the Tf-Idf vectors)\n",
    "* pandas (to process the job and resume data)\n",
    "* cosine_similarity (to calculate the cosine similarity between two vectors)\n",
    "\n",
    "\n",
    "\n",
    "## Task 1: Parsing Job Postings\n",
    "This task comprises of extracting the job related information for a job posting. The extracted data has to be written to a json and xml file of a similar structure.\n",
    "\n",
    "The task was achieved in the following steps:\n",
    "- Read the job posting file job_postings_raw.dat.\n",
    "- Analysed the data to identify the patterns in the data.\n",
    "- Designed regex and other logics to extract most of the data effeciently.\n",
    "- Write extracted data to json and xml\n",
    "\n",
    "More details for each task will be given in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hSwtmHOnh6iy"
   },
   "source": [
    "## Step 1 - Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8TXTQl94c5XN"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZWmOj_XiyByx"
   },
   "source": [
    "## Step 2 - Reading job postings file\n",
    "\n",
    "- Reading the file based on the separator for each job posting, thus 'data' is a list of all the job postings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SPJxvR17c5XU"
   },
   "outputs": [],
   "source": [
    "job_file=open('./input/job_postings_raw.dat', \"r\")\n",
    "data=job_file.read().split(\"------------------------------\")\n",
    "job_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "piSYFBSt0mJq"
   },
   "source": [
    "## Step 3 XML writer\n",
    "\n",
    "#### Workflow\n",
    "- XML String creator\n",
    "  - Takes a dictionary as input\n",
    "    - Checks if the type of the input is dict or list\n",
    "    - Replacing all escaped characters in xml by their equivaelnt according to the specifications\n",
    "    - Recursively calling the method to go to any depth of children, and executing the same\n",
    "  - Returns string object with indentation in xml like format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HUnFTwXHc5Yo"
   },
   "outputs": [],
   "source": [
    "def xml_writer(json_object, indent=\"\"):\n",
    "    result_list = list()\n",
    "    json_object_type = type(json_object)\n",
    "    \n",
    "    xml_escape_char={'&':'&amp;',\n",
    "                    '<':'&lt;',\n",
    "                    '>':'&gt;',\n",
    "                    '\"':'&quot;',\n",
    "                    \"'\":'&apos;'}\n",
    "    \n",
    "    \n",
    "    if json_object_type is list:\n",
    "        for elem in json_object:\n",
    "            result_list.append(xml_writer(elem, indent))\n",
    "        return \"\\n\".join(result_list)\n",
    "\n",
    "    if json_object_type is dict:\n",
    "        for tag_name in json_object:\n",
    "            child = json_object[tag_name]\n",
    "            if not isinstance(child, list):\n",
    "                for key, value in xml_escape_char.items():\n",
    "                    child=re.sub(key,value,child)\n",
    "            result_list.append(\"%s<%s>\" % (indent, tag_name))\n",
    "            result_list.append(xml_writer(child, \"\\t\" + indent))\n",
    "            result_list.append(\"%s</%s>\" % (indent, tag_name))\n",
    "\n",
    "\n",
    "        return \"\\n\".join(result_list)\n",
    "\n",
    "    return \"%s%s\" % (indent, json_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCYbCTtH2S9R"
   },
   "source": [
    "## Step 4 - Parsing the file\n",
    "\n",
    "#### Workflow\n",
    "- Created a dictionary of regex replacements\n",
    "  - Replacing all the found variations of the each tag with a normalised version of the form '^key\\r', i.e. left padding it with '^' and right padding it with '\\r'\n",
    "  - Variations found\n",
    "    - \"^start_date\\r\" -> (DATE_START|START DATE|START_DA|start_date|DATES)\n",
    "    - \"^required_qualifications\\r\" -> \"(REQ_QUALS|QUALIFICATION|qualifications|QUALIFS|REQUIRED QUALIFICATIONS)\"\n",
    "    - \"^job_responsibilities\\r\" -> \"(JOBRESPONSIBILITIES|RESPONSIBILITY|JOB_RESPS|RESP|responsibilities)\"\n",
    "    - \"^title\\r\" -> \"(JOB TITLE|JOB_T|\\_TTL|TITLES|title)\"\n",
    "    - \"^location\\r\" -> \"(JOB_LOC|\\_LOC|\\_LOCS|LOCATION|LOCATIONS)\"\n",
    "    - \"^job_descriptions\\r\" -> \"(JOB DESCRIPTION|JOB_DESC|job_desc|\\_description|DESCRIPTION)\"\n",
    "    - \"^salary\\r\" -> \"(JOB_SAL|REMUNERATION|SALARY|remuneration|salary)\"\n",
    "    - \"^application_procedure\\r\" -> \"(JOB_PROC|PROCEDURE|PROCEDURES|procedures|JOB_PROCS)\"\n",
    "    - \"^application_deadline\\r\" -> \"(APPLICATION_DEADL|APPLICATION_DL|DEAD_LINE|DEADLINES|deadline)\"\n",
    "    - \"^about_company\\r\" -> (ABOUT COMPANY|COMPANYS_INFO|about_company|\\_info|ABOUT)\n",
    "\n",
    "- Removing the unneccessary tags such as \"(REMUNERATION\\/|OPEN TO\\/|START DATE\\/|ABOUT PROGRAM\\/)\" which are making the data dirty\n",
    "- Iterating every job posting\n",
    "  - Checking for headers with children and applying regex and child creation logic and performing extraction\n",
    "      - Job Description:\n",
    "            Identified the separator of each description child as being double quotes. The data has to be cleaned before being able to extract the children. Cleaning the data for the job description by replacing three double quotes with one double quote and two single quotes, then replacing the two double quotes with two single quotes, and then splitting the data on a single double quote. Then replacing multiple occurences of 'NA' with a single value of 'N/A' inserted as the child. Populating that into the dictionaries maintained for xml and json writing\n",
    "      - Job Responsibilities & Required Qualifications:\n",
    "            Identified these two headers to be separated majorly on the basis of '-' as bullet and ';' or '.' which marks the end of each child. Few variations were there in which the child end was marked by new line or comma, which has been handled conditionally. The regex used for extracting the children is '(?:(?!- ).)\\*(?=\\\\n|$)'. The '(?:)' specifies the capturing group and the '(?!)' specifies the negative lookahead, thus it will capture eveyrthing after the negative lookahead expression specified including the line breaks as specified by the DOTALL modifier, till it finds a new line or end of line followed by the negative lookahead in the non capturing group. Similarly the other variations of the regex just has the end of child different extracting similar type of data with the oberved variations.\n",
    "  - Extracting id separately\n",
    "  - Extracting all other tags\n",
    "  - Write each record to xml file for every iteration\n",
    "  - Populate a each listing to dictionary for json writer\n",
    "- Write the populated dictionary for json writer to the json file\n",
    "    \n",
    "                   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NRKcdeSUc5Yq",
    "outputId": "cc6762a4-9254-40b8-ff9e-c6e2a7e8aa4b"
   },
   "outputs": [],
   "source": [
    "#Initialising a listing dictionary to store the values for writing the json file\n",
    "listings = {'listings': {\"listing\":[]}}\n",
    "\n",
    "# Creating a headers regex dict to replace the irregular headers with the normalised ones\n",
    "headers_regex_dict = {\"\\\\n(DATE_START|START DATE|START_DA|start_date|DATES):\": \"^start_date\\r\", \n",
    "                      \"\\\\n(REQ_QUALS|QUALIFICATION|qualifications|QUALIFS|REQUIRED QUALIFICATIONS):\": \"^required_qualifications\\r\",\n",
    "                      \"\\\\n(JOB RESPONSIBILITIES|RESPONSIBILITY|JOB_RESPS|RESP|responsibilities):\":\"^job_responsibilities\\r\",\n",
    "                     \"\\\\n(JOB TITLE|JOB_T|_TTL|TITLES|title):\":\"^title\\r\",\n",
    "                     \"\\\\n(JOB_LOC|_LOC|_LOCS|LOCATION|LOCATIONS):\":\"^location\\r\",\n",
    "                     \"\\\\n(JOB DESCRIPTION|JOB_DESC|job_desc|_description|DESCRIPTION):\":\"^job_descriptions\\r\",\n",
    "                     \"\\\\n(JOB_SAL|REMUNERATION|SALARY|remuneration|salary):\":\"^salary\\r\",\n",
    "                     \"\\\\n(JOB_PROC|PROCEDURE|PROCEDURES|procedures|JOB_PROCS):\":\"^application_procedure\\r\",\n",
    "                     \"\\\\n(APPLICATION_DEADL|APPLICATION_DL|DEAD_LINE|DEADLINES|deadline):\":\"^application_deadline\\r\",\n",
    "                     \"\\\\n(ABOUT COMPANY|COMPANYS_INFO|about_company|_info|ABOUT):\":\"^about_company\\r\",\n",
    "                     \"(ID):\":\"id\\r\"}\n",
    "\n",
    "# Initialsing the list of tags to be used in further dictionaries\n",
    "list_of_xml_tags=[\"title\",\"location\",\"job_descriptions\",\"job_responsibilities\",\"required_qualifications\",\"salary\",\n",
    "              \"application_procedure\",\"start_date\",\"application_deadline\",\"about_company\"]\n",
    "list_of_nones = [None] * 10\n",
    "list_of_json_tags=['_id']+list_of_xml_tags\n",
    "\n",
    "# opening the xml file to write data\n",
    "file_obj=open('./output/job_postings_extracted.xml', 'w')\n",
    "# writing initial lines according to the xml specs\n",
    "file_obj.write('<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\\n')\n",
    "file_obj.write('<listings>\\n')\n",
    "\n",
    "n=len(data)-1\n",
    "# iterating each job posting\n",
    "for i in range(0,n):\n",
    "    # Replacing the dirty tags, cleansing the data\n",
    "    text=re.sub('\\\\n(REMUNERATION\\/|OPEN TO\\/|START DATE\\/|ABOUT PROGRAM\\/)\\\\n','\\n',data[i])\n",
    "    \n",
    "    # Normalising the headers using the dict defined before\n",
    "    for key, value in headers_regex_dict.items():\n",
    "        text=re.sub(key,value,text)\n",
    "    \n",
    "    # Splitting each key from other keys\n",
    "    tag_split=text.split('^')\n",
    "    \n",
    "    # Initialsing the dictionaries to be used to maintain the xml and the json formats\n",
    "    dictionary_json = dict(zip(list_of_json_tags, list_of_nones))\n",
    "    dictionary_xml = dict(zip(list_of_xml_tags, list_of_nones))\n",
    "    \n",
    "    # Iterating the list of keys along with their values\n",
    "    for element_tag in tag_split:\n",
    "        # Extracting the key\n",
    "        header=element_tag.split('\\r')[0].strip()\n",
    "        # Extracting the value\n",
    "        value=element_tag.split('\\r')[1].strip()\n",
    "                \n",
    "        # Checking for key which is of child form\n",
    "        if header in ['job_descriptions','required_qualifications','job_responsibilities']:\n",
    "            \n",
    "            if header == 'job_descriptions':\n",
    "              # Cleaning the data in the job descriptions\n",
    "                cleaned_value=re.sub('\"\"\"',\"\\\"''\",value)\n",
    "                # Replacing two double quotes with two single quotes to not lose the data\n",
    "                cleaned_value=re.sub('\"\"',\"''\",cleaned_value)\n",
    "                # Cleansing the data\n",
    "                d=re.sub('\\n',\" \",cleaned_value).split('\"')\n",
    "                description_list=re.sub('\\n',\" \",cleaned_value).split('\"')\n",
    "                # extracting the list of probable job descriptions after splitting\n",
    "                description_list=[x for x in description_list if x.strip() if x!=',']\n",
    "\n",
    "                # iterating each job description and re-inserting 'N/A' for multiple occurences for 'NA'\n",
    "                desc_list=[]\n",
    "                for d in description_list:\n",
    "                    if d.count(',NA,')>0:\n",
    "                        desc_list.append('N/A')\n",
    "                    else:\n",
    "                      # using the similar logic above to maintain integrity of the data \n",
    "                        desc_list.append(re.sub(\"''\",'\"',d))\n",
    "                \n",
    "                # initialising the json form of the current key and populating it\n",
    "                json_desc={'description':None}\n",
    "                \n",
    "                # Proceed only if list not empty\n",
    "                if desc_list:\n",
    "                \n",
    "                    json_desc['description']=desc_list\n",
    "                    dictionary_json[header]=json_desc\n",
    "\n",
    "                    # initialising the xml form of the current key and populating it\n",
    "                    list_of_desc_dict=[]\n",
    "                    for d in desc_list:\n",
    "                        desc_dict={'description':None}\n",
    "                        desc_dict['description']=d\n",
    "                        list_of_desc_dict.append(desc_dict)\n",
    "                    dictionary_xml[header]=list_of_desc_dict\n",
    "            \n",
    "            else:\n",
    "                # Check if header is required qual\n",
    "                if header=='required_qualifications':\n",
    "                    header_key='qualification'\n",
    "                    \n",
    "                else:\n",
    "                # Check if header is job resposnibility\n",
    "                    header_key='responsibility'\n",
    "                \n",
    "                \n",
    "                # Using regex to extract the text starting with '-' until ';' or '.'\n",
    "                #cleansing the data by removing tabs and extra new line breaks\n",
    "                value=re.sub('\\\\t',' ',value)\n",
    "                value=re.sub('\\\\n[\\w\\/ ]+:[ ]*\\\\n','\\\\n',value)\n",
    "                \n",
    "                #checking if the semi colon is present in the data according to my observation \n",
    "                #that probably some other separator\n",
    "                # separates the child\n",
    "                if value.find(';')==-1:\n",
    "                    # two variations were also observed\n",
    "                    # one ending with comma\n",
    "                    if value.find(',\\\\n')==-1:\n",
    "                        qualifications=re.findall('(?:(?!- ).)*(?=\\\\n|$)',value,re.M|re.S)\n",
    "                    #not ending with any special character but with a new line break\n",
    "                    else:\n",
    "                        qualifications=re.findall('(?:(?!- ).)*(?=[,\\.])',value,re.M|re.S)\n",
    "                        \n",
    "                # child ending with semi colon\n",
    "                else:\n",
    "                    qualifications=re.findall('(?:(?!- ).)*(?=[;\\.])',value,re.M|re.S)\n",
    "        \n",
    "                # removing new line breaks from extracted value list\n",
    "                qualifications=[re.sub('\\n',' ',q.strip()) for q in qualifications if q]\n",
    "                \n",
    "                # initialising the json form of the current key and populating it\n",
    "                json_req_qual={header_key:None}\n",
    "                \n",
    "                # Proceed only if list not empty\n",
    "                if qualifications:\n",
    "                    json_req_qual[header_key]=qualifications\n",
    "                    dictionary_json[header]=json_req_qual\n",
    "                \n",
    "                    # initialising the xml form of the current key and populating it\n",
    "                    list_of_qual_dict=[]\n",
    "                    for qual in qualifications:\n",
    "                        qual_dict={header_key:None}\n",
    "                        qual_dict[header_key]=qual\n",
    "                        list_of_qual_dict.append(qual_dict)\n",
    "                    dictionary_xml[header]=list_of_qual_dict\n",
    "                \n",
    "        elif header == 'id':\n",
    "            # writing 'id' value to xml file\n",
    "            file_obj.write(\"\\t<listing id='\"+value.strip()+\"'>\\n\")\n",
    "            # storing id to json dict for processing later\n",
    "            dictionary_json['_id']=re.sub('\\n',' ',value).strip()\n",
    "        \n",
    "        else:\n",
    "            # extracting all other tags using same logic\n",
    "            dictionary_json[header]=dictionary_xml[header]=re.sub('\\n',' ',value).strip()\n",
    "     \n",
    "    \n",
    "    # replacing the empty or None values with N/A in json dict\n",
    "    for k, v in dictionary_json.items():\n",
    "        if v is None or not v:\n",
    "            dictionary_json[k] = \"N/A\"\n",
    "    \n",
    "    # replacing the empty or None values with N/A in xml dict\n",
    "    for k, v in dictionary_xml.items():\n",
    "        if v is None or not v:\n",
    "            dictionary_xml[k] = \"N/A\"\n",
    "    \n",
    "    # Creating the json object of the xml_dictionary and passing it to xml_writer to prepare the xml string\n",
    "    j = json.loads(json.dumps(dictionary_xml))\n",
    "    file_obj.write(xml_writer(j,'\\t\\t')) # writing the returned xml formatted string\n",
    "    file_obj.write(\"\\n\\t</listing>\\n\") # ending current listing\n",
    "    \n",
    "    # appending the current listing to json dictionary to be used later\n",
    "    listings['listings'][\"listing\"].append(dictionary_json)\n",
    "    \n",
    "# ending the root of the xml\n",
    "file_obj.write(\"</listings>\")\n",
    "# closing the xml file\n",
    "file_obj.close()\n",
    "# opening the json file\n",
    "json_file_obj=open('./output/job_postings_extracted.json', 'w')\n",
    "#dumping the json dictionary to the file using indent and pretty print\n",
    "json.dump(listings, json_file_obj, sort_keys=False, indent=4)\n",
    "#closing th json file\n",
    "json_file_obj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 Summary\n",
    "\n",
    "This task measured the understanding of parsing dirty data of huge numbers using Python, not using external packages. The main outcomes were:\n",
    "\n",
    "- **Reading file** - Reading a single file of that many lines at a point had to made effecient as the buffer memory could not allocatte so much for reading the whole data at once.\n",
    "- **Designing Regex** - Scanned through the raw data file to find variations and patterns which could help form the regex to effeciently extract most chunks of data.\n",
    "- **Extracting data** - Processing the records and extracting them one by one using the regex designed.\n",
    "- **Processing 30K+ records** - Effectively processed the records in a very small amount of time approximately 1K records in a second.\n",
    "- **Writing structured output files** - Writing the extracted data to the JSON and XML structured format, using well defined tags and child separators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "35i33jL2c-U5"
   },
   "source": [
    "## Task 2: Parsing Resume Files\n",
    "This task comprises of extracting the candidate related information from a resume file. The extracted data has to be used to create a vocab and a count vector.\n",
    "\n",
    "The task was achieved in the following steps:\n",
    "- Read the resume dataset file containing the resumes to process\n",
    "- Read the relevant resumes\n",
    "- Apply the tokenization, word removals and stemming to produce good vocab\n",
    "- Write the extracted vocab in a file and its relevant resume file and related count in another\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hSwtmHOnh6iy"
   },
   "source": [
    "## Step 1 - Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "526SK6paOdlk"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import defaultdict,Counter\n",
    "import math\n",
    "import string\n",
    "from itertools import chain\n",
    "from nltk.collocations import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZWmOj_XiyByx"
   },
   "source": [
    "## Step 2 - Read Files\n",
    "\n",
    "- Reading resume_dataset file to extract my relevant resume nos\n",
    "- Creating a raw resume dictionary to store all the data related to the resume nos provided to me\n",
    "- Checking if any resume is empty removing that resume key from the dictionary\n",
    "- Reading the stopwords from the file provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rFYmo4JVTKlM"
   },
   "outputs": [],
   "source": [
    "#reading the common file to extract the resume nos related to my student id\n",
    "resume_dataset_file=open ('./input/resume_dataset.txt', \"r\")\n",
    "resume_dataset=resume_dataset_file.read().split(\"\\n\\n\")\n",
    "resume_dataset_file.close()\n",
    "my_resume_nos=list(set(re.findall('\\d+',[x for x in resume_dataset if '29262909' in x][0])[1:]))\n",
    "\n",
    "#extracting the information related to one resume in a dictionary format\n",
    "resume_raw={}\n",
    "for res_no in my_resume_nos: \n",
    "    file = open('./input/resumeTxt/resume_('+res_no+').txt','r',encoding='UTF-8')\n",
    "    resume_raw[res_no]=file.read()\n",
    "    file.close()\n",
    "\n",
    "# Removing all the keys which have their values empty\n",
    "empty_keys=[]\n",
    "for k,v in resume_raw.items():\n",
    "    if v.strip() =='':\n",
    "        empty_keys.append(k)\n",
    "for x in empty_keys:\n",
    "    resume_raw.pop(x, None)\n",
    "# Reading the stopwords from the given file\n",
    "file = open('./input/stopwords_en.txt','r')\n",
    "stopwords_list=file.read().split()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VTd_HdK5Z2FA"
   },
   "source": [
    "## Step 3 - Defining the functions\n",
    "\n",
    "- lower_repl() function is used to convert the word after a new line or full stop to lower\n",
    "- generate_200_bigrams() is used to generate the required bigrams for the vocab\n",
    "- clean_data() is used to clean the original resume content by removing non-printable characters, extra spaces and extra lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wFa1kTvKOdlp"
   },
   "outputs": [],
   "source": [
    "# function to convert the second group of the match to lower, used for lower casing the words after '\\n' or '.'\n",
    "def lower_repl(match):\n",
    "     return match.group(1) + match.group(2).lower()\n",
    " \n",
    "  #function to generate the meaningful 200 bigrams from a input dictionary of values containing tokens \n",
    "def generate_200_bigrams(input_dict):\n",
    "    # making one list of tokens from the multiple token list for each key\n",
    "    all_resume_tokenised_words = list(chain.from_iterable(input_dict.values()))\n",
    "    # Crating an object for the BigramAssocMeasures \n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    # used to find the bigram collocations from a list of words\n",
    "    bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(all_resume_tokenised_words)\n",
    "    # rejects bigrams which have less than 10\n",
    "    bigram_finder.apply_freq_filter(10)\n",
    "    # filter those words which are of lenth less than 3 or is a stopword\n",
    "    bigram_finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in stopwords_list)\n",
    "    #finds the top n bigrams using the Pointwise Mutual Information association measure\n",
    "    top_200_bigrams = bigram_finder.nbest(bigram_measures.pmi, 200)\n",
    "    #returns the top 200 bigrams\n",
    "    return top_200_bigrams\n",
    "\n",
    "# function to the clean the data in the resume, given the resume no\n",
    "def clean_data(res_num):\n",
    "    res_content=resume_raw[res_num]\n",
    "    #clean the data for the tokens which cannot be identified or some bullets\n",
    "    content_cleaned=re.sub('[\u0004–”“‘’_]','',res_content)\n",
    "    # clean the data by replacing multiple '\\n' with a single '\\n'\n",
    "    content_cleaned=re.sub('\\\\n( *(\\\\n)+)+','\\\\n',content_cleaned)\n",
    "    # clean the data by replacing more than one space with a single space\n",
    "    content_cleaned=re.sub(' +',' ',content_cleaned)\n",
    "    # cleaning the data by replacing space or '\\n' after a full stop by a single space after it\n",
    "    content_cleaned=re.sub(\"\\. +\\\\n *\",'. ',content_cleaned)\n",
    "    # extracting the word after a '\\n' or a full stop to normalise the case\n",
    "    content_cleaned=re.sub('(\\\\n *|\\. +)(\\w+)',lower_repl,content_cleaned)\n",
    "    # replacing all escape sequences with single space\n",
    "    content_cleaned=re.sub(string.whitespace,' ',content_cleaned)\n",
    "    return content_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xeLBN-OTbFOM"
   },
   "source": [
    "## Step 4 - Creating the Vocab\n",
    "\n",
    "1. Normalising the case of the tokens which are at the beginning of a sentence or line, using the dictionary data structure {res_no:res_tokens}\n",
    "2. Generating the top 200 meaningful bigrams\n",
    "3. Using MWETokenizer to retokenise the original tokens again to remove the split words which are now in the bigrams\n",
    "4. Removing the context independent stop words and short words\n",
    "5. Removing the context dependent stop words and rare tokens based on threshold document frequency\n",
    "6. Stemming the remaining vocab using PorterStemmer\n",
    "7. Write the vocab into the vocab file with the following dictionary data structure {word:word_index}\n",
    "8. Count the term frequency of the vocab items and write to the count vector file with the dictionary {res_no: {word1:count1, word2:count2,...}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xeLBN-OTbFOM"
   },
   "source": [
    "### Step 4.1 - Cleaning and Normalising\n",
    "The cleaned data is tokenised using the given word regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-AGylqrUOdlw",
    "outputId": "951b250b-a639-493d-ad30-4836cfe95024"
   },
   "outputs": [],
   "source": [
    "#the word tokenizer provided in the specs\n",
    "word_tokenizer = RegexpTokenizer(r\"\\w+(?:[-']\\w+)?\")\n",
    "\n",
    "# using the tokenizer on each resume content after cleaning with lowercasing\n",
    "resume_tokenised={}\n",
    "for res_num in resume_raw.keys():\n",
    "    resume_tokenised[res_num]=word_tokenizer.tokenize(clean_data(res_num).strip())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xeLBN-OTbFOM"
   },
   "source": [
    "### Step 4.2 - Finding the top 200 Meaningful bigrams\n",
    "The 200 meaningful bigrams are generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the top 200 meaningful bigrams  \n",
    "top_200_bigrams=generate_200_bigrams(resume_tokenised)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xeLBN-OTbFOM"
   },
   "source": [
    "### Step 4.3 - Retokenizing the original tokens using MWETokenizer\n",
    "Retokenising to ensure that the split words are excluded from the tokens using the multi word tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the MWETokenizer to retokenise the list of tokens before generating the bigrams, so as to exclude the split words\n",
    "mwe_tokenizer = MWETokenizer(top_200_bigrams)\n",
    "resume_c={}\n",
    "for key in resume_tokenised.keys():\n",
    "    resume_c[key]=mwe_tokenizer.tokenize(resume_tokenised[key])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xeLBN-OTbFOM"
   },
   "source": [
    "### Step 4.4 - Filtering out the stop words and short words\n",
    "Removing the stop words and the short words from the list of tokens for each resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing context independent stop words and short words\n",
    "resume_stopped={}\n",
    "for key in resume_c.keys():\n",
    "    resume_stopped[key]=[x for x in resume_c[key] if len(x)>=3 if x.lower() not in stopwords_list]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xeLBN-OTbFOM"
   },
   "source": [
    "### Step 4.5 - Removing the rare and common tokens\n",
    "Calculating the threshold frequencies of the word and removing the ones which appear in less than 2% or more than 98% of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing tokens which are not between 2% and 98% of document frequency\n",
    "#Initialising an integer dicitonary to count the document frequency\n",
    "doc_freq=defaultdict(int) \n",
    "for res_no in resume_stopped.keys():\n",
    "    for word in set(resume_stopped[res_no]):\n",
    "        doc_freq[word.lower()]+=1\n",
    "#creating a list of tokens not within that document frequency\n",
    "rare_common_tokens=[]\n",
    "for k in doc_freq.keys():\n",
    "    if doc_freq[k]>0.98*len(resume_raw.keys()) or doc_freq[k]<0.02*len(resume_raw.keys()):\n",
    "                    rare_common_tokens.append(k)\n",
    "# filtering out those tokens which are not there within that range of document frequency\n",
    "resume_frequent={}\n",
    "for key in resume_stopped.keys():\n",
    "    resume_frequent[key]=[x for x in resume_stopped[key] if x.lower() not in rare_common_tokens]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xeLBN-OTbFOM"
   },
   "source": [
    "### Step 4.6 - Stemming the tokens using Porter Stemmer\n",
    "Stemming the tokens before combining all the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "                    \n",
    "#Stemming the words using the porter stemmer\n",
    "stemmer=PorterStemmer()\n",
    "resume_stemmed={}\n",
    "for key in resume_frequent.keys():\n",
    "    resume_stemmed[key]=[stemmer.stem(x) for x in resume_frequent[key]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xeLBN-OTbFOM"
   },
   "source": [
    "### Step 4.7 - Finding out the term frequency for each resume\n",
    "Calculating the term frequency for each term in a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the term frequency of each term in a document using the collections.Counter class\n",
    "resume_words = []\n",
    "resume_term_freq={}\n",
    "for key in resume_stemmed.keys():\n",
    "    resume_words+=resume_stemmed[key]\n",
    "    resume_term_freq[key]=dict(Counter(resume_stemmed[key]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xeLBN-OTbFOM"
   },
   "source": [
    "### Step 4.8 - Creating a dictionary of words and their indices\n",
    "Making a vocab with words and their index positions after being sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of the sorted set of all the words with their index to be written in the vocab\n",
    "vocab_dict={word:index for index, word in enumerate(sorted(set(resume_words)))}    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xeLBN-OTbFOM"
   },
   "source": [
    "### Step 4.9 - Writing the vocab of word:word_index form\n",
    "Writing the vocab file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the vocab file and write the vocab_dict in the specified format\n",
    "vocab_file=open('./output/resume_vocab.txt','w')\n",
    "for word,index in vocab_dict.items():\n",
    "    vocab_file.write(\"%s:%s\\n\" % (word, index))\n",
    "vocab_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xeLBN-OTbFOM"
   },
   "source": [
    "### Step 4.10 - Writing the countVec file \n",
    "Writing the countVec file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#open the count vector file and write the resume_term_freq in the specified format\n",
    "countVec_file=open('./output/resume_countVec.txt','w')\n",
    "for key,tfd in resume_term_freq.items():\n",
    "    countVec_file.write(\"%s\" % 'resume_('+key+')')\n",
    "    for term, frequency in tfd.items():\n",
    "        countVec_file.write(\", %s:%s\" % (vocab_dict[term], frequency))\n",
    "    countVec_file.write(\"\\n\")\n",
    "countVec_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 Summary\n",
    "\n",
    "This task measured the understanding of extracting meaningful information from a large number of files using Python and NLTK. The main outcomes acheived were:\n",
    "\n",
    "- **Pre-processing the data** - The data should be cleaned before processing just to extract meaningful information much effectively.\n",
    "- **Generating Bigrams and MWETokenizer** - Generating bigrams based on the corpus and using multi word tokenizer to tokenise them.\n",
    "- **Filtering certain type of words** - Filter of certain type of words which are not useful for analysis such as stop words, common tokens, rare tokens and short tokens,\n",
    "- **Calculating term frequency and document frequency** - The calculation of term frequency and document frequency can be further used to provide recommendations.\n",
    "- **Stemming the words** - Rooting the word is important as the variations in the language can make two contextually similar words appear different although their essence is same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "35i33jL2c-U5"
   },
   "source": [
    "## Task 3: Ranking Resumes with respect to job advertisements\n",
    "The task comprises of recommending top 10 resumes that fit best for the first 500 job advertisements in task 1 with respect to their required qualifications.\n",
    "\n",
    "The steps to achieve the task are as follows:\n",
    "- From the vocab.txt and countVec.txt, read the resume ids and their content, and convert it to a dataframe of columns ['ID','Corpus']\n",
    "- From the job_postings_extracted.json.json, read the first 500 job postings which have the 'required qualifications' field in them, similarly convert that into a dataframe consisting of job ids and their content as ['ID','Corpus']\n",
    "- After creating the job and resume dataframes, adding each job to a copy of the resume dataframe, and using that combined dataframe to find the tf-idf vector and correspondingly finding the vector cosine similarity of the job with respect to all the resumes and using it to determine the top 10 resumes having the highest similarity in the cosine value.\n",
    "- Writing the recommended resumes for a job to the file.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hSwtmHOnh6iy"
   },
   "source": [
    "## Step 1 - Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z65nFvaw3KAA"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import defaultdict,Counter\n",
    "import math\n",
    "import string\n",
    "from itertools import chain\n",
    "from nltk.collocations import *\n",
    "import itertools\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZWmOj_XiyByx"
   },
   "source": [
    "## Step 2 - Read Files\n",
    "\n",
    "- Reading stopwords_en.txt file to extract the stopwords\n",
    "- Reading the resume_vocab.txt file to extract all the resume vocab from task 2\n",
    "- Reading the resume_countVec.txt file to extract the related resume and the word index of the vocab from task 2\n",
    "- Reading the job_postings_extracted.json file to extract all the job postings from task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yco448cR3KAF"
   },
   "outputs": [],
   "source": [
    "#initialising the tokeniser for words as specified in task2\n",
    "word_tokenizer = RegexpTokenizer(r\"\\w+(?:[-']\\w+)?\")\n",
    "\n",
    "# reading the stopwords file\n",
    "file = open('./input/stopwords_en.txt','r')\n",
    "stopwords_list=file.read().split()\n",
    "file.close()\n",
    "\n",
    "# reading the vocab file\n",
    "read_vocab_file=open(\"./output/resume_vocab.txt\",'r')\n",
    "vocab_data=read_vocab_file.read().split('\\n')\n",
    "read_vocab_file.close()\n",
    "\n",
    "\n",
    "# reading the countVec file\n",
    "read_countVec_file=open(\"./output/resume_countVec.txt\",'r')\n",
    "countVec_data=read_countVec_file.read().split('\\n')\n",
    "read_countVec_file.close()\n",
    "\n",
    "# reading the job posting output json file\n",
    "read_json=open(\"./output/job_postings_extracted.json\",'r')\n",
    "jobs_json=json.loads(read_json.read())\n",
    "read_json.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rd1ChFXq9XFN"
   },
   "source": [
    "## Step 3 - Creating the resume dataframe\n",
    "\n",
    "- Iterate the vocab file data to make a vocab dictionary\n",
    "- Iterate the countVec file data to make a dictionary of the form {resumeid:resumecontent} using the vocab dictionary\n",
    "- Use the dictionary from the above step to create a resume dataframe of the columns ['ID','Corpus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pHEttz_b3KAI",
    "outputId": "dbfbd892-a2ea-42e2-bd04-c7002bc079e9"
   },
   "outputs": [],
   "source": [
    "# a placeholder for all the vocab and their indices\n",
    "vocab_dict={}\n",
    "for v in vocab_data[:len(vocab_data)-1]:\n",
    "    vocab_dict[v.split(':')[1]]=v.split(':')[0]\n",
    "\n",
    "# a dictionary for all the resume and its content in key value pairs\n",
    "resume_dict={}\n",
    "for res in countVec_data[:len(countVec_data)-1]:\n",
    "    resume_string=''\n",
    "    #splitting the content on comma\n",
    "    count_list=res.split(',')\n",
    "    #extracting the resume id from the data using group\n",
    "    key=re.search('\\d+',count_list[0]).group(0)\n",
    "    #iterating each word index and using the vocab dict to find the actual word\n",
    "    for c in count_list[1:]:\n",
    "        word_index=c.split(':')[0]\n",
    "        word_count=int(c.split(':')[1])\n",
    "        #performing the lookup by using the word index\n",
    "        word_text=vocab_dict[word_index.strip()]\n",
    "        # using the frequency of the word to form the string of words for each resume\n",
    "        for i in range(0,word_count):\n",
    "            resume_string+=' '+ word_text\n",
    "    #setting the content value for a resume key\n",
    "    resume_dict[key]=resume_string.strip()\n",
    "\n",
    "#Transforming the above formed dictionary to a dataframe\n",
    "resume_df=pd.DataFrame.from_dict(resume_dict, orient='index').reset_index()\n",
    "#Renaming the columns of the dataframe\n",
    "resume_df.rename(columns={'index': 'ID', 0: 'Corpus'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wU49sy_H-uBX"
   },
   "source": [
    "## Step 4 - Creating the job dataframe\n",
    "\n",
    "- Extracting the first 500 job postings which have the neccessary field for recommendation, i.e. the required qualification field\n",
    "- Tokenise the job content with the same regex used in task 2, find bigrams, remove short words and stop words, stem the tokens, thus making the tokens similar to that of the resumes\n",
    "- Create a job dataframe['ID':'Corpus'] from the dictionary {jobid:jobcontent}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wUA2M00J3KAd",
    "outputId": "91e6f6c3-b0f5-48df-82ab-047715f57786"
   },
   "outputs": [],
   "source": [
    "# bigram function to generate the bigrams for the job postings similar to that of the resumes in task2\n",
    "def generate_200_bigrams(input_dict):\n",
    "    # making one list of tokens from the multiple token list for each key\n",
    "    all_resume_tokenised_words = list(chain.from_iterable(input_dict.values()))\n",
    "    # Crating an object for the BigramAssocMeasures \n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    # used to find the bigram collocations from a list of words\n",
    "    bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(all_resume_tokenised_words)\n",
    "    # rejects bigrams which have less than 10\n",
    "    bigram_finder.apply_freq_filter(10)\n",
    "    # filter those words which are of lenth less than 3 or is a stopword\n",
    "    bigram_finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in stopwords_list)\n",
    "    #finds the top n bigrams using the Pointwise Mutual Information association measure\n",
    "    top_200_bigrams = bigram_finder.nbest(bigram_measures.pmi, 200)\n",
    "    #returns the top 200 bigrams\n",
    "    return top_200_bigrams\n",
    "  \n",
    "# using the json input to extract out all the job postings\n",
    "list_of_jobs=jobs_json['listings']['listing']\n",
    "job_data={}\n",
    "ctr=0\n",
    "# iterating each job posting to check whether it has the header needed for performing recommendation\n",
    "for j in list_of_jobs:\n",
    "    if j['required_qualifications']!='N/A':\n",
    "        job_data[j['_id']]=j['required_qualifications']['qualification']\n",
    "        ctr+=1\n",
    "    if ctr==500:#till we find 500 job postings\n",
    "        break\n",
    "    \n",
    "#Performing the same operations on the job posting as done in task 2 for resumes\n",
    "\n",
    "#tokenise the job content\n",
    "job_tokenised={}\n",
    "for k in job_data.keys():\n",
    "    job_tokenised[k]=list(chain.from_iterable([word_tokenizer.tokenize(x) for x in job_data[k]]))\n",
    "\n",
    "#find the bigrams for the job data\n",
    "job_top_200_bigrams=generate_200_bigrams(job_tokenised)\n",
    "#use the mwetokenizer to avoid the split words of bigrams to be in the corpus\n",
    "job_mwe_tokenizer = MWETokenizer(job_top_200_bigrams)\n",
    "job_c={}\n",
    "for key in job_tokenised.keys():\n",
    "    job_c[key]=job_mwe_tokenizer.tokenize(job_tokenised[key])\n",
    "\n",
    "#Removing the stopwords and short words\n",
    "job_stopped={}\n",
    "for key in job_c.keys():\n",
    "    job_stopped[key]=[x for x in job_c[key] if len(x)>=3 if x.lower() not in stopwords_list]\n",
    "\n",
    "# Stemming the stopped tokens to be similar to the ones after task2 for resumes    \n",
    "stemmer=PorterStemmer()\n",
    "job_stemmed={}\n",
    "for key in job_stopped.keys():\n",
    "    job_stemmed[key]=[stemmer.stem(x) for x in job_stopped[key]]\n",
    "\n",
    "#Combining the set of tokens into a single string for each job     \n",
    "jids=[]\n",
    "job_words = []\n",
    "job_term_freq={}\n",
    "for key in job_stemmed.keys():\n",
    "    jids.append(key)\n",
    "    job_words.append(' '.join(job_stemmed[key]))\n",
    "    job_term_freq[key]=dict(Counter(job_stemmed[key]))\n",
    "\n",
    "# Making the job ids and their content into the dataframe similar to that of the previous step for the resume\n",
    "job_df=pd.DataFrame.from_dict(dict(zip(jids,job_words)), orient='index').reset_index()\n",
    "job_df.rename(columns={'index': 'ID', 0: 'Corpus'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c-u_-bHgAIaH"
   },
   "source": [
    "## Step 5 - Finding the best fit resumes related to each job\n",
    "\n",
    "- Iterating each job in job dataframe\n",
    "  - make a tf-idf vector after combining that job with resume corpus\n",
    "  - find the cosine similarity of that job with all other resumes\n",
    "  - extract the most similar ones for each job and write to a dictionary of the form {jobid:[resumeid1,resumeid2,...]}\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UEeei_MC3KAh",
    "outputId": "ef4bf373-4a9b-4993-af56-7aa55cd060f7"
   },
   "outputs": [],
   "source": [
    "# creating a placehoder for the recommended resumes for each job\n",
    "rec_dict={}\n",
    "# initialising the tfidf vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(input = 'content', analyzer = 'word')\n",
    "#Iterating each job in the job dataframe\n",
    "for index, row in job_df.iterrows():\n",
    "    \n",
    "    #combining the single job record with the resumes \n",
    "    job_resume_df=resume_df\n",
    "    job_resume_df.loc[-1]=row\n",
    "    job_resume_df.index+=1\n",
    "    job_resume_df.sort_index(inplace=True)\n",
    "    \n",
    "    #creating tf-idf vectors for the combined corpus\n",
    "    tfidf_vectors = tfidf_vectorizer.fit_transform(job_resume_df['Corpus'])\n",
    "    #finding the cosine similarities for the job with respect to the other resumes\n",
    "    cosine_similarities = cosine_similarity(tfidf_vectors[0:1],tfidf_vectors)\n",
    "    # sorting the smilarities and extracting the indices which are having more similarity\n",
    "    res_indices=cosine_similarities[0].argsort()[:-12:-1]\n",
    "    # creating a dictionary of job id as key and the list of top ten recommended resumes for each job\n",
    "    rec_dict[row['ID']]=[job_resume_df['ID'][i] for i in res_indices][1:]\n",
    "    #dropping the current job record for further iterations\n",
    "    job_resume_df.drop(job_resume_df.index[0],inplace=True)\n",
    "    job_resume_df.reset_index(drop=True,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hxrQHI_NBOez"
   },
   "source": [
    "## Step 5 - Writing the bonus output file\n",
    "\n",
    "- Using the dictionary from the above step, to write the bonus output file in the same format as mentioned in the specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ym1sgVJN3KAs"
   },
   "outputs": [],
   "source": [
    "# writing the recommended resumes for each job to the file\n",
    "bonus_file=open('./output/ranked_resumes.txt','w')\n",
    "for key in rec_dict.keys():\n",
    "    bonus_file.write(key+':'+','.join(rec_dict[key])+'\\n')\n",
    "bonus_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mVCNsWv93KAv"
   },
   "source": [
    "## Task 3 Summary\n",
    "\n",
    "This task of the assessment measured the understaning of using a recommender system to recommend top resumes for a job. The main outcomes achieved were:\n",
    "\n",
    "- **Generating Tf-Idf vectors** - To find the similarity the corpus has to be generated using the tf-idf vectorizer, and the required profiling to be done of the resumes. \n",
    "- **Cosine Similarity** - This cosine similarity helps us to find the similarity between the vectors based on the cosine of the angle formed between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1TQ9QMVMc-XD"
   },
   "source": [
    "# References\n",
    "\n",
    "- Python Software Foundation. (2018). *Python Documentation*. Retrieved from https://docs.python.org/3.6/\n",
    "- Regex101. (2018). *Quick Reference*. Retrieved from https://regex101.com/\n",
    "- Python Software Foundation. (2018). *Python Documentation*. Retrieved from https://docs.python.org/3.6/\n",
    "- Regex101. (2018). *Quick Reference*. Retrieved from https://regex101.com/\n",
    "- NLTK Project. (2018). *Collocations*. Retrieved from http://www.nltk.org/howto/collocations.html\n",
    "- NLTK 3.3 documentation. (2018). *Association Measures*. Retrieved from http://www.nltk.org/_modules/nltk/metrics/association.html\n",
    "- The `pandas` Project. (2018). *pandas 0.23.4 documentation: pandas.DataFrame*. Retrived from https://pandas.pydata.org/pandas-docs/stable/\n",
    "- Python Software Foundation. (2018). *Python Documentation*. Retrieved from https://docs.python.org/3.6/\n",
    "- Wikipedia. (2018). *Cosine similarity*. Retrieved from https://en.wikipedia.org/wiki/Cosine_similarity\n",
    "- Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011. (2018). *sklearn.metrics.pairwise.cosine_similarity*. Retrieved from http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html\n",
    "- Perone, Christian. (2013, September 12). *Machine Learning :: Cosine Similarity for Vector Space Models (Part III)* Retrieved from http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/\n",
    "- Raman Venkat. (2017, October 23). *How To Build a Simple Content Based Book Recommender System* https://www.linkedin.com/pulse/content-based-recommender-engine-under-hood-venkat-raman/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
